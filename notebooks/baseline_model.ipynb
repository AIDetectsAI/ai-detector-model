{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02e6faf",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "Poniżej znajduje się skrypt do trenowania modelu. W katalogu znajduje się również checkpoint do załadowania dla aktualnej wersji. Model jest jeszcze niedopracowany i wymaga badania nad architekturą sieci i datasetem.\n",
    "\n",
    "Użyte dane: \n",
    "1. https://www.kaggle.com/datasets/tristanzhang32/ai-generated-images-vs-real-images - Cały dataset treningowy\n",
    "2. https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images - po 10000 zdjęć do AI i Naturalnych do podzbioru treningowego i po 3000 do testowego (w celu dostarczenia zdjęć z niską rozdzielczością)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fc4621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device 0: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"CUDA device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No CUDA devices available.\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f240aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "KAGGLE_USERNAME = os.getenv('KAGGLE_USERNAME')\n",
    "KAGGLE_KEY = os.getenv(\"KAGGLE_KEY\")\n",
    "\n",
    "os.environ['KAGGLEHUB_CACHE'] = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1728d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ratattwg/Desktop/testing_baseline_model/data/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ratattwg/miniconda3/envs/py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "\n",
    "path = kagglehub.dataset_download(\"tristanzhang32/ai-generated-images-vs-real-images\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7500538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "train_transform = transforms.Compose([  \n",
    "    transforms.RandomResizedCrop(128),  # Randomly crop to 128x128\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])  # Normalize to [-1, 1]\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = dsets.ImageFolder(root=os.path.join(path, \"train\"), transform=train_transform)\n",
    "test_dataset = dsets.ImageFolder(root=os.path.join(path, \"test\"), transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=12, # Ustawić do wartości odpowiadającej liczbie wątków CPU\n",
    "    pin_memory=True,\n",
    "    pin_memory_device=device,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=12, # Analogicznie do train_loader\n",
    "    pin_memory=True,\n",
    "    pin_memory_device=device,\n",
    "    prefetch_factor=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc3c62a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x761670f400e0>, <torch.utils.data.dataloader.DataLoader object at 0x761668a30b00>)\n",
      "Length of train dataloader: 532 batches of 128\n",
      "Length of test dataloader: 141 batches of 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataloaders: {train_loader, test_loader}\") \n",
    "print(f\"Length of train dataloader: {len(train_loader)} batches of {128}\")\n",
    "print(f\"Length of test dataloader: {len(test_loader)} batches of {128}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fb480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Two 3×3 convs with batchnorm and ReLU, plus skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        skip = self.skip(x)\n",
    "        return self.relu(out + skip)\n",
    "\n",
    "class CustomBinaryCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN for AI vs. natural image classification.\n",
    "    - 4 residual convolutional stages\n",
    "    - SpatialDropout2d for regularization\n",
    "    - Global average pooling\n",
    "    - Small classification head\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ResidualBlock(3, 32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ResidualBlock(32, 64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ResidualBlock(64, 128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "        self.stage4 = nn.Sequential(\n",
    "            ResidualBlock(128, 256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(x)    # [B,32,H/2,W/2]\n",
    "        x = self.stage2(x)    # [B,64,H/4,W/4]\n",
    "        x = self.stage3(x)    # [B,128,H/8,W/8]\n",
    "        x = self.stage4(x)    # [B,256,H/16,W/16]\n",
    "        x = self.global_pool(x)  # [B,256,1,1]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = self(x)\n",
    "            return self.sigmoid(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ce849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (features, labels) in enumerate(test_loader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model.predict(features)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_outputs.extend(outputs.view(-1).cpu().numpy())\n",
    "\n",
    "            #batch_accuracy = np.mean((outputs.view(-1).numpy() > 0.5) == labels.numpy())\n",
    "            #batch_auc = roc_auc_score(labels.numpy(), outputs.view(-1).numpy())\n",
    "            #print(f\"Batch {i+1}/{len(test_loader)}, Loss: {loss_value.item():.4f}, Accuracy: {batch_accuracy:.4f}, AUC: {batch_auc:.4f}\")\n",
    "    \n",
    "    accuracy = np.mean((np.array(all_outputs) > 0.5) == np.array(all_labels))\n",
    "    \n",
    "    auc_score = roc_auc_score(all_labels, all_outputs)\n",
    "    precision = precision_score(all_labels, (np.array(all_outputs) > 0.5).astype(int))\n",
    "    recall = recall_score(all_labels, (np.array(all_outputs) > 0.5).astype(int))\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenowanie modelu\n",
    "\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "model = CustomBinaryCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss_value = loss(outputs.view(-1), labels.float())\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        preds = (outputs.view(-1) > 0.0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss_value.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_model(model, test_loader, loss)\n",
    "        torch.save(model.state_dict(), f\"./model_epoch_{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca248200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ratattwg/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/ratattwg/miniconda3/envs/py312/lib/python3.12/site-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (143040000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ratattwg/miniconda3/envs/py312/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/ratattwg/miniconda3/envs/py312/lib/python3.12/site-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (121554000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/ratattwg/miniconda3/envs/py312/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/ratattwg/miniconda3/envs/py312/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/ratattwg/miniconda3/envs/py312/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/ratattwg/miniconda3/envs/py312/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8327, AUC: 0.9156, Precision: 0.8101, Recall: 0.8696\n"
     ]
    }
   ],
   "source": [
    "# Walidacja załadowanego modelu\n",
    "\n",
    "model = CustomBinaryCNN().to(device)\n",
    "model.load_state_dict(torch.load(\"./baseline_model.pth\", map_location=torch.device('cuda')))\n",
    "\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "test_model(model, test_loader, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440039b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
