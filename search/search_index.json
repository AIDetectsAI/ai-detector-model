{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ai-detector-model documentation! Description A machine learning model designed to classify images as either AI-generated or real, using visual features and patterns to distinguish synthetic content from authentic photographs. Commands The Makefile contains the central entry points for common tasks related to this project.","title":"Home"},{"location":"#ai-detector-model-documentation","text":"","title":"ai-detector-model documentation!"},{"location":"#description","text":"A machine learning model designed to classify images as either AI-generated or real, using visual features and patterns to distinguish synthetic content from authentic photographs.","title":"Description"},{"location":"#commands","text":"The Makefile contains the central entry points for common tasks related to this project.","title":"Commands"},{"location":"getting-started/","text":"Getting started This is where you describe how to get set up on a clean install, including the commands necessary to get the raw data (using the sync_data_from_s3 command, for example), and then how to make the cleaned, final data sets.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This is where you describe how to get set up on a clean install, including the commands necessary to get the raw data (using the sync_data_from_s3 command, for example), and then how to make the cleaned, final data sets.","title":"Getting started"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/","text":"Metryki w klasyfikacji binarnej Ten dokument opisuje najcz\u0119\u015bciej stosowane metryki w klasyfikacji binarnej, ich znaczenie, spos\u00f3b obliczania oraz interpretacj\u0119 nietypowych warto\u015bci (np. du\u017cy accuracy przy z\u0142ym logloss ). Celem jest u\u0142atwienie zrozumienia wynik\u00f3w modelu i ich potencjalnych implikacji (np. przetrenowanie, z\u0142a kalibracja, niezr\u00f3wnowa\u017cone dane). Macierz pomy\u0142ek (Confusion Matrix) Macierz pomy\u0142ek to podstawowy spos\u00f3b wizualizacji dzia\u0142ania modelu klasyfikacyjnego: Predykcja: 0 (negatywna) Predykcja: 1 (pozytywna) Prawda: 0 TN \u2013 True Negative FP \u2013 False Positive Prawda: 1 FN \u2013 False Negative TP \u2013 True Positive Na podstawie tej macierzy obliczamy wi\u0119kszo\u015b\u0107 metryk klasyfikacyjnych. Kluczowe metryki 1. Accuracy (dok\u0142adno\u015b\u0107) Opis : Procent poprawnie sklasyfikowanych przyk\u0142ad\u00f3w. Wz\u00f3r : $$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$ Uwagi : Mo\u017ce by\u0107 myl\u0105ce w przypadku niezr\u00f3wnowa\u017conych danych, wi\u0119c trzeba uwa\u017ca\u0107 z jego stosowaniem. 2. Precision (precyzja) Opis : Mierzy jak wiele pozytywnych predykcji jest poprawnych. Wz\u00f3r : $$ \\text{Precision} = \\frac{TP}{TP + FP} $$ Zastosowanie : Minimalizacja fa\u0142szywych alarm\u00f3w. 3. Recall (czu\u0142o\u015b\u0107) Opis : Jak wiele rzeczywistych pozytywnych przyk\u0142ad\u00f3w zosta\u0142o wykrytych? Wz\u00f3r: $$ \\text{Recall} = \\frac{TP}{TP + FN} $$ Zastosowanie : Minimalizacja pomini\u0119\u0107 (FN). 4. F1 Score Opis: Metryka kt\u00f3ra mierzy dok\u0142adno\u015b\u0107 modelu balansuj\u0105c precyzj\u0119 i czu\u0142o\u015b\u0107. Wz\u00f3r: $$ F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} $$ Zalety : R\u00f3wnowa\u017cy precyzj\u0119 i czu\u0142o\u015b\u0107. 5. Log Loss (Logarithmic Loss) Opis: Miara r\u00f3\u017cnicy mi\u0119dzy przewidywanymi prawdopodbie\u0144stwami a rzeczywistymi warto\u015bciami przy klasyfikacji. Wz\u00f3r: $$ \\text{LogLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)] $$ Zalety : Uwzgl\u0119dnia skalibrowanie predykcji (pewno\u015b\u0107 modelu). Wysoki logloss + dobre accuracy : wskazuje na z\u0142\u0105 kalibracj\u0119 modelu. 6. AUC - ROC Opis : Prawdopodobie\u0144stwo, \u017ce model prawid\u0142owo rozr\u00f3\u017cni klas\u0119 pozytywn\u0105 od negatywnej. AUC to pole pod krzyw\u0105 ROC. Wz\u00f3r: $$ \\text{AUC} = \\int_{0}^{1} TPR(FPR) \\, dFPR $$ Interpretacja : 0.5 = klasyfikator losowy 1.0 = idealny Zalety : Niezale\u017cna od progu, odporna na niezbalansowane dane. 7. Brier Score Opis : Mierzy \u015bredni kwadratowy b\u0142\u0105d mi\u0119dzy przewidywanym prawdopodobie\u0144stwem a etykiet\u0105. Wz\u00f3r :$$ \\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^N (p_i - y_i)^2 $$ Zakres : od 0.0 (idealnie skalibrowany) do 1.0 (kompletnie b\u0142\u0119dny). Zastosowanie : Ocena kalibracji modeli probabilistycznych. Mniej agresywny ni\u017c log loss \u2013 nie karze tak bardzo za wysok\u0105 pewno\u015b\u0107 b\u0142\u0119dnej predykcji. Uwaga : niski Brier Score sugeruje dobre skalibrowanie predykcji, nawet je\u015bli accuracy jest przeci\u0119tne. Typowe przypadki i interpretacje Sytuacja Mo\u017cliwa interpretacja Wysokie Accuracy + Wysoki LogLoss Dobre klasy, z\u0142a kalibracja (niepewno\u015b\u0107 predykcji). Niska Precision, wysoka Recall Du\u017co wykrytych przypadk\u00f3w, ale te\u017c wiele fa\u0142szywych alarm\u00f3w. Wysoki F1, niski AUC Model dzia\u0142a dobrze przy danym progu, ale globalnie s\u0142abo rozr\u00f3\u017cnia klasy. Niski LogLoss, niski Accuracy Model pewny siebie, ale cz\u0119sto si\u0119 myli \u2013 mo\u017ce sugerowa\u0107 \u017ale dobrane cechy lub pr\u00f3g. R\u00f3\u017cnice mi\u0119dzy trenowaniem a walidacj\u0105 Mo\u017cliwe przetrenowanie modelu. Przyk\u0142ad praktyczny Za\u0142\u00f3\u017cmy \u017ce model zosta\u0142 przetestowany na 1000 przyk\u0142adach: Predykcja: 0 Predykcja: 1 Prawda: 0 850 (TN) 50 (FP) Prawda: 1 40 (FN) 60 (TP) Obliczone metryki: Accuracy = 91% Precision \u2248 54.5% Recall = 60% F1 Score \u2248 0.57 LogLoss \u2013 zak\u0142adamy wysoki z powodu niepewnych predykcji AUC \u2013 umiarkowany Wniosek : Model dobrze przewiduje klasy, ale ma z\u0142\u0105 kalibracj\u0119 \u2013 wskazane u\u017cycie wykresu kalibracji i ewentualna korekta (np. Platt scaling). Kod: obliczanie metryk i wizualizacji (Python + scikit-learn) ```python import numpy as np from sklearn.metrics import ( accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, calibration_curve, ) import matplotlib.pyplot as plt Przyk\u0142adowe dane y_true = np.array([0] 850 + [0] 50 + [1] 40 + [1] 60) # 1000 przyk\u0142ad\u00f3w y_pred = np.array([0] 850 + [1] 50 + [0] 40 + [1] 60) # binarne predykcje Probabilistyczne predykcje (symulacja niepewno\u015bci) y_prob = np.concatenate([ np.random.uniform(0.1, 0.4, 850), # TN np.random.uniform(0.6, 0.9, 50), # FP np.random.uniform(0.1, 0.4, 40), # FN np.random.uniform(0.6, 0.9, 60), # TP ]) Obliczanie metryk print(\"Accuracy:\", accuracy_score(y_true, y_pred)) print(\"Precision:\", precision_score(y_true, y_pred)) print(\"Recall:\", recall_score(y_true, y_pred)) print(\"F1 Score:\", f1_score(y_true, y_pred)) print(\"Log Loss:\", log_loss(y_true, y_prob)) print(\"ROC AUC:\", roc_auc_score(y_true, y_prob)) Macierz pomy\u0142ek cm = confusion_matrix(y_true, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=cm) disp.plot(cmap=plt.cm.Blues) plt.title(\"Confusion Matrix\") plt.show() Krzywa kalibracji prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10) plt.figure(figsize=(6, 6)) plt.plot(prob_pred, prob_true, marker='o', label='Model') plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Ideal Calibration') plt.title('Calibration Curve') plt.xlabel('\u015arednie przewidywane prawdopodobie\u0144stwo') plt.ylabel('Rzeczywisty odsetek klasy 1') plt.legend() plt.grid() plt.show()","title":"Metryki w klasyfikacji binarnej"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#metryki-w-klasyfikacji-binarnej","text":"Ten dokument opisuje najcz\u0119\u015bciej stosowane metryki w klasyfikacji binarnej, ich znaczenie, spos\u00f3b obliczania oraz interpretacj\u0119 nietypowych warto\u015bci (np. du\u017cy accuracy przy z\u0142ym logloss ). Celem jest u\u0142atwienie zrozumienia wynik\u00f3w modelu i ich potencjalnych implikacji (np. przetrenowanie, z\u0142a kalibracja, niezr\u00f3wnowa\u017cone dane).","title":"Metryki w klasyfikacji binarnej"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#macierz-pomyek-confusion-matrix","text":"Macierz pomy\u0142ek to podstawowy spos\u00f3b wizualizacji dzia\u0142ania modelu klasyfikacyjnego: Predykcja: 0 (negatywna) Predykcja: 1 (pozytywna) Prawda: 0 TN \u2013 True Negative FP \u2013 False Positive Prawda: 1 FN \u2013 False Negative TP \u2013 True Positive Na podstawie tej macierzy obliczamy wi\u0119kszo\u015b\u0107 metryk klasyfikacyjnych.","title":"Macierz pomy\u0142ek (Confusion Matrix)"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#kluczowe-metryki","text":"","title":"Kluczowe metryki"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#1-accuracy-dokadnosc","text":"Opis : Procent poprawnie sklasyfikowanych przyk\u0142ad\u00f3w. Wz\u00f3r : $$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$ Uwagi : Mo\u017ce by\u0107 myl\u0105ce w przypadku niezr\u00f3wnowa\u017conych danych, wi\u0119c trzeba uwa\u017ca\u0107 z jego stosowaniem.","title":"1. Accuracy (dok\u0142adno\u015b\u0107)"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#2-precision-precyzja","text":"Opis : Mierzy jak wiele pozytywnych predykcji jest poprawnych. Wz\u00f3r : $$ \\text{Precision} = \\frac{TP}{TP + FP} $$ Zastosowanie : Minimalizacja fa\u0142szywych alarm\u00f3w.","title":"2. Precision (precyzja)"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#3-recall-czuosc","text":"Opis : Jak wiele rzeczywistych pozytywnych przyk\u0142ad\u00f3w zosta\u0142o wykrytych? Wz\u00f3r: $$ \\text{Recall} = \\frac{TP}{TP + FN} $$ Zastosowanie : Minimalizacja pomini\u0119\u0107 (FN).","title":"3. Recall (czu\u0142o\u015b\u0107)"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#4-f1-score","text":"Opis: Metryka kt\u00f3ra mierzy dok\u0142adno\u015b\u0107 modelu balansuj\u0105c precyzj\u0119 i czu\u0142o\u015b\u0107. Wz\u00f3r: $$ F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} $$ Zalety : R\u00f3wnowa\u017cy precyzj\u0119 i czu\u0142o\u015b\u0107.","title":"4. F1 Score"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#5-log-loss-logarithmic-loss","text":"Opis: Miara r\u00f3\u017cnicy mi\u0119dzy przewidywanymi prawdopodbie\u0144stwami a rzeczywistymi warto\u015bciami przy klasyfikacji. Wz\u00f3r: $$ \\text{LogLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)] $$ Zalety : Uwzgl\u0119dnia skalibrowanie predykcji (pewno\u015b\u0107 modelu). Wysoki logloss + dobre accuracy : wskazuje na z\u0142\u0105 kalibracj\u0119 modelu.","title":"5. Log Loss (Logarithmic Loss)"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#6-auc-roc","text":"Opis : Prawdopodobie\u0144stwo, \u017ce model prawid\u0142owo rozr\u00f3\u017cni klas\u0119 pozytywn\u0105 od negatywnej. AUC to pole pod krzyw\u0105 ROC. Wz\u00f3r: $$ \\text{AUC} = \\int_{0}^{1} TPR(FPR) \\, dFPR $$ Interpretacja : 0.5 = klasyfikator losowy 1.0 = idealny Zalety : Niezale\u017cna od progu, odporna na niezbalansowane dane.","title":"6. AUC - ROC"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#7-brier-score","text":"Opis : Mierzy \u015bredni kwadratowy b\u0142\u0105d mi\u0119dzy przewidywanym prawdopodobie\u0144stwem a etykiet\u0105. Wz\u00f3r :$$ \\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^N (p_i - y_i)^2 $$ Zakres : od 0.0 (idealnie skalibrowany) do 1.0 (kompletnie b\u0142\u0119dny). Zastosowanie : Ocena kalibracji modeli probabilistycznych. Mniej agresywny ni\u017c log loss \u2013 nie karze tak bardzo za wysok\u0105 pewno\u015b\u0107 b\u0142\u0119dnej predykcji. Uwaga : niski Brier Score sugeruje dobre skalibrowanie predykcji, nawet je\u015bli accuracy jest przeci\u0119tne.","title":"7. Brier Score"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#typowe-przypadki-i-interpretacje","text":"Sytuacja Mo\u017cliwa interpretacja Wysokie Accuracy + Wysoki LogLoss Dobre klasy, z\u0142a kalibracja (niepewno\u015b\u0107 predykcji). Niska Precision, wysoka Recall Du\u017co wykrytych przypadk\u00f3w, ale te\u017c wiele fa\u0142szywych alarm\u00f3w. Wysoki F1, niski AUC Model dzia\u0142a dobrze przy danym progu, ale globalnie s\u0142abo rozr\u00f3\u017cnia klasy. Niski LogLoss, niski Accuracy Model pewny siebie, ale cz\u0119sto si\u0119 myli \u2013 mo\u017ce sugerowa\u0107 \u017ale dobrane cechy lub pr\u00f3g. R\u00f3\u017cnice mi\u0119dzy trenowaniem a walidacj\u0105 Mo\u017cliwe przetrenowanie modelu.","title":"Typowe przypadki i interpretacje"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#przykad-praktyczny","text":"Za\u0142\u00f3\u017cmy \u017ce model zosta\u0142 przetestowany na 1000 przyk\u0142adach: Predykcja: 0 Predykcja: 1 Prawda: 0 850 (TN) 50 (FP) Prawda: 1 40 (FN) 60 (TP)","title":"Przyk\u0142ad praktyczny"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#obliczone-metryki","text":"Accuracy = 91% Precision \u2248 54.5% Recall = 60% F1 Score \u2248 0.57 LogLoss \u2013 zak\u0142adamy wysoki z powodu niepewnych predykcji AUC \u2013 umiarkowany Wniosek : Model dobrze przewiduje klasy, ale ma z\u0142\u0105 kalibracj\u0119 \u2013 wskazane u\u017cycie wykresu kalibracji i ewentualna korekta (np. Platt scaling).","title":"Obliczone metryki:"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#kod-obliczanie-metryk-i-wizualizacji-python-scikit-learn","text":"```python import numpy as np from sklearn.metrics import ( accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, calibration_curve, ) import matplotlib.pyplot as plt","title":"Kod: obliczanie metryk i wizualizacji (Python + scikit-learn)"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#przykadowe-dane","text":"y_true = np.array([0] 850 + [0] 50 + [1] 40 + [1] 60) # 1000 przyk\u0142ad\u00f3w y_pred = np.array([0] 850 + [1] 50 + [0] 40 + [1] 60) # binarne predykcje","title":"Przyk\u0142adowe dane"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#probabilistyczne-predykcje-symulacja-niepewnosci","text":"y_prob = np.concatenate([ np.random.uniform(0.1, 0.4, 850), # TN np.random.uniform(0.6, 0.9, 50), # FP np.random.uniform(0.1, 0.4, 40), # FN np.random.uniform(0.6, 0.9, 60), # TP ])","title":"Probabilistyczne predykcje (symulacja niepewno\u015bci)"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#obliczanie-metryk","text":"print(\"Accuracy:\", accuracy_score(y_true, y_pred)) print(\"Precision:\", precision_score(y_true, y_pred)) print(\"Recall:\", recall_score(y_true, y_pred)) print(\"F1 Score:\", f1_score(y_true, y_pred)) print(\"Log Loss:\", log_loss(y_true, y_prob)) print(\"ROC AUC:\", roc_auc_score(y_true, y_prob))","title":"Obliczanie metryk"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#macierz-pomyek","text":"cm = confusion_matrix(y_true, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=cm) disp.plot(cmap=plt.cm.Blues) plt.title(\"Confusion Matrix\") plt.show()","title":"Macierz pomy\u0142ek"},{"location":"experiments/Metryki%20klasyfikacji%20binarnej/#krzywa-kalibracji","text":"prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10) plt.figure(figsize=(6, 6)) plt.plot(prob_pred, prob_true, marker='o', label='Model') plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Ideal Calibration') plt.title('Calibration Curve') plt.xlabel('\u015arednie przewidywane prawdopodobie\u0144stwo') plt.ylabel('Rzeczywisty odsetek klasy 1') plt.legend() plt.grid() plt.show()","title":"Krzywa kalibracji"}]}